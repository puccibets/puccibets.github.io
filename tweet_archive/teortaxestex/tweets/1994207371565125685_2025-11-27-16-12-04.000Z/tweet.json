{
  "id": "1994207371565125685",
  "url": "https://x.com/teortaxesTex/status/1994207371565125685",
  "timestamp": "2025-11-27T16:12:04.000Z",
  "capturedAt": "2025-11-28T20:06:44.541Z",
  "author": "Teortaxes (DeepSeek 推特铁粉 2023 – ∞) @teortaxesTex",
  "authorName": "Teortaxes (DeepSeek 推特铁粉 2023 – ∞)",
  "authorHandle": "@teortaxesTex",
  "text": "I strongly condemn dunking on Prime Intellect, they're doing the exact right thing.\n\nPost-training Chinese base models to the frontier level is in fact *more important* right now than learning to pretrain our own bases. I basically don't care what PI, Arcee and others can pretrain, though I have reasonable expectations that they'll catch up soon. Compute is abundant in the West and we already see evidence of sufficient pretraining expertise with smaller models (these two + @ZyphraAI, @Dorialexander, @natolambert with Olmo…) in the Western open space; by all accounts it scales. But that's mostly of… geopolitical significance, of what you guys will be allowed to run on your patriotic servers plugged into agentic frameworks. I'm not Western nor Chinese, and contrary to my posting, I don't care terminally about this dimension, it's a purely instrumental issue. Consult the bio: the race is not between the US/West and China, it's between humans and AGIs vs ape power centralization. And Prime Intellect is doing more than anyone to arrest the centralizing drive.\n\nConsider and weep: HF is chock full of Celestial gifts that we're too inept to utilize, they just rot there until they become obsolete. Thousands to millions of downloads and nothing to show. Why is Qwen even doing antiquated, very expensive Llama-like dense models in the first place? Mostly because a) Alibaba has a KPI \"monthly HF downloads\" and b) academics and small labs can't figure out how to finetune modern architectures. Even were the infrastructure more mature and they less technically ngmi, what do they finetune it on? The narrative peak of open source finetuning was Nous-Hermes, and that paradigm was basically just distilling GPT-4, filtering according to \"taste\" and vague criteria, SFTing over a strong base, and hoping for the best. That angle of attack was scornfully dismissed in advance by OpenAI et al as a non-threatening dead end that rewards hallucinations and style mimicking, and it predictably fizzled out. What next, «RL»? What RL, how RL, what is the signal generator, how does it intersect with downstream tasks? Kimi-K2, an immaculate frontier-level base, has been available to all for many months. DeepSeek-V3, nearly a year now. V2, well over a year. Dozens of models in all sizes, periodically updated with longer context and other boons. And what have we built with all that? \nAnything that even approaches Chinese in-house Instructs, nevermind contemporary frontier? Hello? Can you point me to these derivatives? It's a complete profanation of the idea of open science. And not even the Chinese bother, they all just train their own models from scratch. I can think of a tiny number of exceptions (eg Rednote making DSV3-VL), but none of them made a big splash. Startups worth billions, whose moat is search or agentic coding and thus large post-training datasets, sneakily use DS/GLM/Qwen in their proprietary products, but they don't share alpha. That's… about it.\n\nEnter Prime Intellect. They're solving training. They're solving environment generation. They're thinking in a principled manner about signals that shape general model cognition. They are, in effect, unlocking the immense store of inert value that had been accumulated. For the world, this is so much more than another me-too model. They're scary smart, they have good intentions, they've got a solid roadmap, and they're my friends. I won't stand for pooh-poohing their work, because it serves the Great Common Task. If you don't see it, you don't have a clue of what's really important at this stage.",
  "avatar": "teortaxestex/avatar/avatar.jpg",
  "avatarPath": "teortaxestex/avatar/avatar.jpg",
  "images": [],
  "jsonFile": "teortaxestex/tweets/1994207371565125685_2025-11-27-16-12-04.000Z/tweet.json",
  "mediaDir": "media",
  "userSlug": "teortaxestex"
}