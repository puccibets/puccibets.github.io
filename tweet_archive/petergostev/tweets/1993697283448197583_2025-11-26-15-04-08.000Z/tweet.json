{
  "id": "1993697283448197583",
  "url": "https://x.com/petergostev/status/1993697283448197583",
  "timestamp": "2025-11-26T15:04:08.000Z",
  "capturedAt": "2025-11-27T20:59:40.525Z",
  "author": "Peter Gostev @petergostev",
  "authorName": "Peter Gostev",
  "authorHandle": "@petergostev",
  "text": "My (speculative) assessment of OpenAI's path, current state & the future:\n\nOpenAI:\n - Right now its 'thinking' model is a lot stronger than anyone else's, while 'non-thinking' model is clearly lagging behind\n - OpenAI discovered o1's inference time compute scaling and changed direction rapidly\n - This was quite a change from the 'scaling pre-train' lab to a 'RL' lab\n - All of the base models for o-series and GPT-5 models are probably trained at a similar level to GPT-4 (Epoch's estimates are showing this too)\n - This means that they haven't meaningfully scaled pre-training for 2.5 years (GPT-4o, GPT-4.1 etc. were all optimisations)\n -  In parallel, GPT-4.5 was the big new pre-train, released 2 years after GPT-4 in March 2025 and OpenAI had big hopes for it\n - But, as GPT-4.5 was sort of a flop and thinking models were so much more impressive, with faster iteration cycles, any new big pre-trains got de-prioritised\n - So GPT-5, 5.1, 5.1-codex etc were all based on probably a new pre-train, maybe a bit bigger than GPT-4, but definitely smaller than GPT-4.5\n\nGoogle & Anthropic:\n - In the meantime, Google and Anthropic haven't worked out the 'reasoning' paradigm (they scrambled after o1-preview) and hence continued refining & scaling pre-training\n - They have slapped on reasoning subsequently, but it is nowhere near as advanced as OpenAI's (e.g. Claude Opus 4.5 SWE bench scores are the same with thinking and without)\n - But, their non-reasoning models are miles ahead of the non-reasoning GPT-5. There's no comparison between Sonnet/Opus 4.5 and GPT-5 without reasoning.\n \nGoing forward:\n - OpenAI is reaching a point where long thinking times become unusable for day to day work, e.g. 10-15 mins for a coding task when Gemini or Claude can do it in 2, eliminates them from a lot of the market, even if the final answer is better\n - Very hard scientific problems will benefit from OpenAI's approach (you can see them talk about science a lot), but this is not where the market is and I don't know how OpenAI can capture the upside of discoveries, if they ever come\n - The question is - does OpenAI have a better pre-train in the back pocket or not? If they do, their response could be fast & mighty\n - If they don't and they have to start now, it would be 6 months+ before we get a big response from OpenAI - 3-4 months for pre-train, 2-3 months for RL, safety etc.\n - The biggest edge I see for OpenAI is for them to leverage their excellent long thinking models for synthetic data generation\n - If they could run models for 5-10-24 hours to get the best data & feed it back to the pre-train, their new base model could be as impressive as Anthropic's & Google's combined\n - Then, imagine Opus 4.5 base + GPT-5-thinking/pro level reasoning, it would be really quite something",
  "avatar": "petergostev/avatar/avatar.jpg",
  "avatarPath": "petergostev/avatar/avatar.jpg",
  "images": [],
  "jsonFile": "petergostev/tweets/1993697283448197583_2025-11-26-15-04-08.000Z/tweet.json",
  "mediaDir": "media",
  "userSlug": "petergostev"
}